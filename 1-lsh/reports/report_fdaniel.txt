The task at hand was to efficiently detect near-duplicate pages (documents). These were given to us in a single text file, where each line represents the equivalent of a a page in shingles. To enable any computations at all, we first parsed the text file, extracted the page id and created an array of corresponding shingles in integer values. Since the whole goal is to use Local Sensitive Hashing in a MapReduce environment, the major design choice in this project was centered around what computations should be done in the mapper and reducer:
We decided that the hashing process and the neccessary computations on the shingles should be done in the mapper, whereas the reducer then checks whether hash collisions exist and computes the similarity of candidate pairs, yielding near-duplicates if they reach or exceed the desired similarity treshold.
Some important implementation details include the fact that we first used MinHashing on the shingles to obtain a signature column per page, which we further separated in bands to apply band hashing next. Relevant constants that we determined empirically and by respecting the given restrictions (max 1024 hash functions) are the number of buckets to be used for band hashing (8193), the number of bands (100) and rows per band (10).
